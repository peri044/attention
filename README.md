# attention
Understanding attention in LLMs

This repo is inspired from https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention and is mostly for self learning the following concepts in detail and with code (and the tensor shapes)

* Self attention (attention.py)
* Multi-head attention (attention.py)
* Causal attention (attention.py) - also known as masked multi-headed attention




