# attention
Understanding attention in LLMs

This repo is inspired from https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention and is mostly for self learning the following concepts in detail and with code (and the tensor shapes)

* Self attention (attention.py)
* Multi-head attention (attention.py)
* Causal attention (attention.py) - also known as masked multi-headed attention


Resources:

* https://github.com/ELS-RD/kernl/tree/main/tutorial - amazing resources on online softmax, flash attention
* Flash attention : https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf


